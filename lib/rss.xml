<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Obsidian Vault]]></title><description><![CDATA[Obsidian digital garden]]></description><link>https://hyunzioh.pages.dev/</link><image><url>https://hyunzioh.pages.dev/lib/media/favicon.png</url><title>Obsidian Vault</title><link>https://hyunzioh.pages.dev/</link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Fri, 19 Jul 2024 05:42:45 GMT</lastBuildDate><atom:link href="https://hyunzioh.pages.dev/lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Fri, 19 Jul 2024 05:42:34 GMT</pubDate><copyright><![CDATA[Hyunzi Oh]]></copyright><ttl>60</ttl><dc:creator>Hyunzi Oh</dc:creator><item><title><![CDATA[Probability, Random Variable, and Distribution]]></title><description><![CDATA[<a class="tag" href="https://hyunzioh.pages.dev/?query=tag:math" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#math</a> <a class="tag" href="https://hyunzioh.pages.dev/?query=tag:measure" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#measure</a> <a class="tag" href="https://hyunzioh.pages.dev/?query=tag:probability" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#probability</a> <a class="tag" href="https://hyunzioh.pages.dev/?query=tag:stat" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#stat</a> <br> <a href="https://hyunzioh.pages.dev?query=tag:math" class="tag is-unresolved" target="_self" rel="noopener" data-href="#math">#math</a> <a href="https://hyunzioh.pages.dev?query=tag:measure" class="tag is-unresolved" target="_self" rel="noopener" data-href="#measure">#measure</a> <a href="https://hyunzioh.pages.dev?query=tag:probability" class="tag is-unresolved" target="_self" rel="noopener" data-href="#probability">#probability</a> <a href="https://hyunzioh.pages.dev?query=tag:stat" class="tag is-unresolved" target="_self" rel="noopener" data-href="#stat">#stat</a><br>Oh, Hyunzi. (email: <a data-tooltip-position="top" aria-label="mailto:wisdom302@naver.com" rel="noopener" class="external-link is-unresolved" href="https://hyunzioh.pages.dev/mailto:wisdom302@naver.com" target="_self">wisdom302@naver.com</a>)<br>
Korea University, Graduate School of Economics.<br>Main References<br>Kim, Dukpa. (2024). "Econometric Analysis" (2024 Spring) ECON 518, Department of Economics, Korea University.
<br>Capinski and Kopp. (2003). "Measure, Integral and Probability" (2nd edition)
<br>Hogg et al. (2013). "Introduction to Mathematical Statistics" (8th Edition)
<br>Here, we briefly introduce more rigorous definitions of the probability theory based on the measure theory. The main goal of this note is to understand the concept of probability measure intuitively, and get familiar with the jargon of measure theory. The detailed theorems and results of the measure theory used in this section will be further analyzed in <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="https://hyunzioh.pages.dev/Measure" class="original-internal-link is-unresolved" target="_self" rel="noopener" style="display: none;">Measure</a><a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="https://hyunzioh.pages.dev/Measure" class="internal-link mathLink-internal-link is-unresolved" target="_self" rel="noopener">Measure</a>, with mathematical proofs.<br>The goal of this note is to fully understand the concepts of the key elements consisting the probability space.<br>Definition (probability space).
A probability space is the triple consists of three elements: <br>A sample space, , is the set of all possible outcomes of a random experiment. An element of is , which is called an outcome.
<br>An event space, , is a collection of all subsets of , called a -field. An element of is , which is called an event.
<br>A probability function (measure), , assigns each event to a probability, which is a number between and .<br>In later chapter <a data-href="#Probability Measure" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#Probability_Measure" class="original-internal-link" target="_self" rel="noopener" style="display: none;">Probability Measure</a><a data-href="#Probability Measure" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#Probability_Measure" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Probability Measure</a>, we will discuss about how to understand <a data-tooltip-position="top" aria-label="^27dce2" data-href="#^27dce2" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^27dce2" class="original-internal-link" target="_self" rel="noopener" style="display: none;">$\sigma-$field</a><a data-tooltip-position="top" aria-label="^27dce2" data-href="#^27dce2" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^27dce2" class="internal-link mathLink-internal-link" target="_self" rel="noopener">field</a> and <a data-tooltip-position="top" aria-label="^58f06f" data-href="#^58f06f" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^58f06f" class="original-internal-link" target="_self" rel="noopener" style="display: none;">measure</a><a data-tooltip-position="top" aria-label="^58f06f" data-href="#^58f06f" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^58f06f" class="internal-link mathLink-internal-link" target="_self" rel="noopener">measure</a>. Before that, we first review some of the key definitions we studied in <a data-tooltip-position="top" aria-label="Statistical Proof" data-href="Statistical Proof" href="https://hyunzioh.pages.dev/Statistical Proof" class="original-internal-link is-unresolved" target="_self" rel="noopener" style="display: none;">Mathematical Statistics</a><a data-tooltip-position="top" aria-label="Statistical Proof" data-href="Statistical Proof" href="https://hyunzioh.pages.dev/Statistical Proof" class="internal-link mathLink-internal-link is-unresolved" target="_self" rel="noopener">Mathematical Statistics</a>.<br>Definition (probability).
A set function on is a probability or probability measure if it satisfies: <br>, for all .
<br>.
<br>If are disjoint, then we have <br>Example (tossing a coin).
Consider an experiment of tossing a coin. The two possible outcomes are head() and tail. Therefore, the sample space is and the event space is By definition, we have , implying that . If the coin is a fair one, then we would also have .
<br>Definition (conditional probability).
For an event such that , the conditional probability of given is defined by which function is also a probability (measure).
<br>Let a probability space be given for the rest of this section.<br>Definition (random variable).
A random variable is a measurable function from to . i.e, it assigns a real number to each outcome.<br>Remark (measureable function).
A function is measurable if where is called the Borel sets.<br>Here, you can simply understand the Borel sets as a collection of subsets of the real line. A detailed explanations follows in <a data-href="Borel Measure" href="https://hyunzioh.pages.dev/Borel Measure" class="original-internal-link is-unresolved" target="_self" rel="noopener" style="display: none;">Borel Measure</a><a data-href="Borel Measure" href="https://hyunzioh.pages.dev/Borel Measure" class="internal-link mathLink-internal-link is-unresolved" target="_self" rel="noopener">Borel Measure</a>. Now we define a probability (measure) on , <br>Example (tossing a coin 2).
In the given sample space we may define a random variable by and .
<br>A random variable is simply a mapping that maps each outcome to a real number where we can use well developed mathematical tools. <br>We now re-define some of the familiar concepts.<br>Definition (distribution).
A distribution of a random variable is the probability (measure) on induced by : <br>Definition (distribution function).
A (cumulative) distribution function of a random variable is defined as If is a continuous random variable, then If is a discrete random variable, then <br>Proposition (properties of distribution function). <br>non-decreasing: <br>limit value: <br>continuity when increasing: <br>expectation: <br>Definition (probability mass function).
A probability mass function (pmf) is the probability distribution of a discrete random variable: <br>The density function of a continuous variable is more tricky to define.<br>Definition (absolutely continuous and density).
If a measure satisfies for every integrable function , then is absolutely continuous. Here such is called as a density of for a measure .
<br>Note that the absolute continuity of is a result from a property of the measure, which itself is continuous. While we will further look into the definition later on, here, you can simply understand as a characteristic of the integral.<br>Definition (probability density function).
A probability density function (pdf) of a continuous random variable is the function that satisfies <br>First, we follow the previous notions of the sample space and the set of all events denoted and , respectively. Obviously, is a collection of subsets of , a set of sets (i.e. family of sets). An typical example of an event set is the power set , which collects all the subsets of .<br>A sigma-algebra is a subset of the poser set , which is therefore a family of sets in , that satisfies certain properties.<br>Definition (sigma-algebra and measurable space).
For a sample space , the set of all events is -algebra or -field on , if the following conditions are satisfied: <br>: inclusion of empty set and the entire set.
<br>, : closed under complements.
<br>, : closed under countable unions. For , which is a -algebra on , an ordered pair of is called a measurable space.<br>Here, note that the term 'sigma-' in mathematics usually implies 'countable union' or 'countably infinite'. Thus, -algebra means it is closed under the countable union, i.e., it contains every countable union of itself. <br>Furthermore, since countable set is isomorphic to , meaning that there exists an one-to-one correspondence between the two, we can understand -algebra as the set consists the countably infinite number of the subsets of , where we can index the each of the subsets in natural number bases.<br>Example (example of sigma-algebra and induced algebra).
Let . Then the -algebra of can be and many others. Here, is a -algebra of induced by the set , meaning that is a -algebra containing .
<br>For some -algebra which is defined on , a measure is a function that assigns each events to a non-negative real numbers: Intuitively, this set function is a generalization and formalization of geometrical measures (length, area, and volume) and other common notions (magnitude, mass, and probability). More simply, we can understand the concept of a measure as a set function that measures the size of the given set.<br>Definition (measure and measure space).
Let be a sample space and be a -algebra over . A set function is called a measure if the following conditions hold: <br>, : non-negativity.
<br>: empty set with measure .
<br>For any disjoint countable collection , of , we have : countable additivity. And a triple is called a measure space.<br>Here, the countable additivity is the key property that makes a measure a generalized length.<br>Proposition (properties of measure).
Let be a measure. The following properties directly follows from the definition. <br>monotonicity: If are measurable sets with , then <br>continuity from below: If are measurable sets that are increasing, then the union of the sets is measurable and <br>continuity from above: If are measurable sets that are decreasing, then the intersection of the sets is measurable. Furthermore, if at least one of the has finite measure then <br>From the definition of <a data-tooltip-position="top" aria-label="^58f06f" data-href="#^58f06f" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^58f06f" class="original-internal-link" target="_self" rel="noopener" style="display: none;">measure</a><a data-tooltip-position="top" aria-label="^58f06f" data-href="#^58f06f" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^58f06f" class="internal-link mathLink-internal-link" target="_self" rel="noopener">measure</a>, by adding one more property, we finally have the definition of the probability measure.<br>Definition (probability measure and probability space).
Let be a sample space and be a -algebra over . A set function is called a probability measure if the following conditions hold: <br>, : non-negativity.
<br>: empty set with measure .
<br>For any disjoint countable collection , of , we have : countable additivity.
<br>: total mass is 1. And the triple is called as a probability space.
<br>Definition (measurable set).
Let be a measurable space. Then a subset is said to be (-)measurable if .<br>Remark (null set).
A measurable set is null set if . <br>Definition (length and null set).
Let a set of open intervals in real line, and define a function such that and denote the function as length. Then, a set is called null set if there exists a sequence of open interval such that for every .
<br>Compared to the empty set, null set refers to the set which is practically non-existing, while empty set denotes the set which is actually empty. Here, the null means meaningless, insignificance, or negligible, rather then the absence.<br>Proposition (theorems of null set). <br>empty set is null set.
<br>any set of a single element is null set.
<br>any countable set is null set. <br>Definition (almost surely).
Let be a probability space. An event happens almost surely if . Equivalently, happens almost surely if the probability of not occurring is zero: .<br>Intuitively, 'almost surely' refers to the every point except the null set. This concept is similar to the integral where , and also to the probability where we does not care whether the end points of the set is included or not.<br>Definition (almost surely converges).
The sequence of random vectors converges almost surely, i.e. if <br>Note that in <a data-href="#^4fba4c" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^4fba4c" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^4fba4c</a><a data-href="#^4fba4c" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^4fba4c" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 24 (almost surely converges)</a>, we do not care about the timing when enters to the neighborhood of and remains there forever. Thus we introduce further concepts.<br>Definition (event eventually and infinitely often).
Let be a probability space, and let be a sequence of events in . Then events eventually (e.v.) denotes the chance of happening in infinite horizon of time: Using De Morgan's law, the opposite case is infinitely often (i.o.), the chance that will not violate the given in infinite horizon of time: <br>Here, the term e.v. and i.o. emphasizes that in terms of convergence in a sequence, the only important thing is the long-term behavior, not the behavior in the first finite horizon of the time. <br>Remark (almost sure convergence, and ev and io).
Define a sequence of sets as . Then, , if the probability that events eventually is equals to : which condition is equivalent to i.e. the probability that happens infinitely often is equals to .<br>Usually, <a data-href="#^2ff43b" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^2ff43b" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^2ff43b</a><a data-href="#^2ff43b" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^2ff43b" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Remark 26 (almost sure convergence, and ev and io)</a> is another common way to define <a data-href="#^4fba4c" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^4fba4c" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^4fba4c</a><a data-href="#^4fba4c" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^4fba4c" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 24 (almost surely converges)</a>. For the brief understanding, suppose pointwise and let . By the definition, for all .<br>
Thus we have Alternatively, resulting in the definition of almost sure convergence. For the infinitely often part, it can be easily driven using De Morgan's law.<br>Intuitively, a measurable function is a function between the two <a data-tooltip-position="top" aria-label="^27dce2" data-href="#^27dce2" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^27dce2" class="original-internal-link" target="_self" rel="noopener" style="display: none;">measurable space</a><a data-tooltip-position="top" aria-label="^27dce2" data-href="#^27dce2" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^27dce2" class="internal-link mathLink-internal-link" target="_self" rel="noopener">measurable space</a> that preserves the structure of the spaces. Here, the inverse image of any measurable function is <a data-tooltip-position="top" aria-label="^b3caaa" data-href="#^b3caaa" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^b3caaa" class="original-internal-link" target="_self" rel="noopener" style="display: none;">measurable</a><a data-tooltip-position="top" aria-label="^b3caaa" data-href="#^b3caaa" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^b3caaa" class="internal-link mathLink-internal-link" target="_self" rel="noopener">measurable</a>. <br>Definition (measurable function).
Let be a measurable space, and for a function , define a set Then, is measurable if for all , .<br>It is shown that it is equivalent to show the following conditions to prove the given function is measurable.<br>Proposition (equivalent conditions to measurable function).
For a function , the following conditions are equivalent: <br>, <br>, <br>, <br>, <br>Proof.As 1 and 2 are complement to each other, similarly, 3 and 4 are complement to each other. By <a data-href="#^27dce2" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^27dce2" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^27dce2</a><a data-href="#^27dce2" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^27dce2" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 14 (sigma-algebra and measurable space)</a>, 1 and 2, and 3 and 4 are equivalent conditions (since the complement set of the element in algebra is also its element). Thus it is sufficient to show that 1 and 3 are equivalent condition, since 1 is a given definition from <a data-href="#^03f1b6" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^03f1b6" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^03f1b6</a><a data-href="#^03f1b6" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^03f1b6" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 27 (measurable function)</a>. Below, we first show that 1 implies 3, and then we show the converse. <br>() Assume for a function , we have Then we have since , and 1 holds for every . Since and field is closed under every intersection, we have .<br>() Assume for a function , we have Then we have and as field is closed under every union. □<br>Remark that <a data-href="#^03f1b6" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^03f1b6" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^03f1b6</a><a data-href="#^03f1b6" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^03f1b6" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 27 (measurable function)</a> is analogue to <a data-href="Basic Topology#^1bc20f" href="https://hyunzioh.pages.dev/Basic Topology#^1bc20f" class="original-internal-link is-unresolved" target="_self" rel="noopener" style="display: none;">Basic Topology &gt; ^1bc20f</a><a data-href="Basic Topology#^1bc20f" href="https://hyunzioh.pages.dev/Basic Topology#^1bc20f" class="internal-link mathLink-internal-link is-unresolved" target="_self" rel="noopener">Basic Topology &gt; Definition 6 (continuous function between topological spaces)</a>, which gives the sense of the preserving properties of the measurable function. <br>For your information, measurable function can be defined in more fundamental way, not directly defining from its inverse-image. In that case, we can also derive the same property of <a data-href="#^03f1b6" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^03f1b6" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^03f1b6</a><a data-href="#^03f1b6" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^03f1b6" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 27 (measurable function)</a>. <br>Furthermore, measurable function can alternatively obtained from the pointwise convergence of the sequence of simple functions.<br>In real space, Borel algebra is the intersection of all <a data-tooltip-position="top" aria-label="^27dce2" data-href="#^27dce2" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^27dce2" class="original-internal-link" target="_self" rel="noopener" style="display: none;">sigma-algebra</a><a data-tooltip-position="top" aria-label="^27dce2" data-href="#^27dce2" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^27dce2" class="internal-link mathLink-internal-link" target="_self" rel="noopener">sigma-algebra</a> that includes open sets in . In other words, it is the smallest sigma-algebra that can be defined on while including all the open sets (intervals) of . It is useful in the sense that it only contains the necessary elements to define a measure on the set of open sets, which makes it available to define a <a data-tooltip-position="top" aria-label="^2c8d7a" data-href="#^2c8d7a" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^2c8d7a" class="original-internal-link" target="_self" rel="noopener" style="display: none;">probability measure</a><a data-tooltip-position="top" aria-label="^2c8d7a" data-href="#^2c8d7a" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^2c8d7a" class="internal-link mathLink-internal-link" target="_self" rel="noopener">probability measure</a>.<br>Definition (Borel sets on Euclidean space).
Let be a sigma-algebra of Euclidean space . Then the sigma-algebra generated by the set of all intervals is said to be Borel sigma-algebra of and is called Borel set.
<br>While we defined Borel algebra under Euclidean space, it is more general to start on an arbitrary set. However, in probability theory, it is sufficient enough to define on , as it consists of all open and closed sets that can be defined on . <br>Example (example of Borel set).
Based on the closure properties of the field, most of the familiar sets in belongs to : <br>By construction, all intervals in belongs to .
<br>Since is a field, and as the all open sets are the countable union of intervals, every open sets in belongs to .
<br>Since each countable sets are a countable union of closed intervals in , every countable sets belongs to . In particular, and are Borel sets.
<br>As a field, includes the complement of a Borel sets, which means, the set of irrational numbers and the finite sets are also Borel sets. <br>Definition (random variable).
Let be a probability space. A function is a random variable if Or, equivalently, we can define as where denotes a Borel algebra.
<br>Note that the equivalence in <a data-href="#^f8eea7" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^f8eea7" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^f8eea7</a><a data-href="#^f8eea7" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^f8eea7" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 31 (random variable)</a> holds by the properties of <a data-href="#^67466e" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^67466e" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^67466e</a><a data-href="#^67466e" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^67466e" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 29 (Borel sets on Euclidean space)</a>. Random variable is a mapping that maps each elements in the sample space into a real space, which makes it available to use inequalities. Also, it restricts the sigma-field to the inverse image of the Borel sets, restraining the excessive abstractness of the sample space. <br>Remark that by the definition, is a ()measurable function. Moreover, if , then becomes a Borel function, since we have .<br>For your information, to generalize the definition into the multivariate random variable, we can simply define such that <br>Definition (sigma-fields generated by random variable).
Let be a probability space and let be a random variable. Then a field defined as is said to be a field generated by . <br>Definition (probability distribution).
Let be a probability space, and let be random variable. Then a measure is said to be probability distribution of if <br>Proposition (countably additive of probability distribution).
The set function is countably additive.
<br>Proof.Let are the pairwise disjoint Borel sets, then their inverse image are also pairwise disjoint. By <a data-href="Measure#^d2709d" href="https://hyunzioh.pages.dev/Measure#^d2709d" class="original-internal-link is-unresolved" target="_self" rel="noopener" style="display: none;">Measure &gt; ^d2709d</a><a data-href="Measure#^d2709d" href="https://hyunzioh.pages.dev/Measure#^d2709d" class="internal-link mathLink-internal-link is-unresolved" target="_self" rel="noopener">Measure &gt; ^d2709d</a>, we have Thus by <a data-href="#^f02a69" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^f02a69" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^f02a69</a><a data-href="#^f02a69" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^f02a69" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 33 (probability distribution)</a>, we have where the third equality holds by <a data-href="#^58f06f" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^58f06f" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^58f06f</a><a data-href="#^58f06f" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^58f06f" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 16 (measure and measure space)</a>, as denotes a probability measure. □<br>Thus, is also a probability space. Note that .<br>Using <a data-href="#^f02a69" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^f02a69" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^f02a69</a><a data-href="#^f02a69" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^f02a69" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 33 (probability distribution)</a>, we can re-define the independence of the random variable using measure theory.<br>Definition (independence of random variable).
Let be a probability space, and let be random variables. We say and are independence if we have <br>Note that the independence between and is equivalent to the followings.<br>Theorem (equivalent of independence).
The following conditions are equivalent. <br>The random variable and are independence. i.e. <br>For all Borel function , we have <br>For the joint distribution , we have <br>Definition (Dirac measure).
Let be a probability space. Assume a random variable is a constant function: for all . Then its probability distribution is said to be Dirac measure : <br>Note that the probability distribution only takes account whether the given is included in the Borel set or not. This approach is fundamentally different than the one based on the difference between the discrete and continuous distribution.<br>Example (multiple value discrete variable).
Consider the case when Then using <a data-href="#^049540" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^049540" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^049540</a><a data-href="#^049540" href="https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html#^049540" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 37 (Dirac measure)</a>, we can express its probability distribution as which means <br>Thus we can now define a general form of discrete probability distribution.<br>Definition (discrete probability distribution).
Let for all , and . Then we can express the probability distribution of a discrete random variable as <br>Classical examples are:<br>Geometric distribution: for some .
<br>Poisson distribution: .
<br>Note that we can also define a random variable that is neither discrete nor continuous. <br>Example (distance between Car and B).
Suppose a car leaves city A at random between 1 pm and 2 pm. It travels at 100 km/h towards B which is 50 km from A. What is the probability distribution of the distance between the car and B at 2 pm?
<br>Proof.If the car starts traveling before 1.30 pm, then it can arrive at B before 2 pm. However, if it starts after 1.30, then its distance from the B would follow a uniform distribution. Thus we have where denotes the minute when the car starts after 1 (i.e. if , then it starts at 1.30 pm).<br>
Then, its probability distribution can be expressed as where denotes a uniform measure. □]]></description><link>https://hyunzioh.pages.dev/mathematical-appendix/probability-theory/probability,-random-variable,-and-distribution.html</link><guid isPermaLink="false">Mathematical Appendix/Probability Theory/Probability, Random Variable, and Distribution.md</guid><dc:creator><![CDATA[Hyunzi Oh]]></dc:creator><pubDate>Thu, 18 Jul 2024 19:21:20 GMT</pubDate></item><item><title><![CDATA[Consumption-CAPM]]></title><description><![CDATA[<a class="tag" href="https://hyunzioh.pages.dev/?query=tag:termstructure" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#termstructure</a> <a class="tag" href="https://hyunzioh.pages.dev/?query=tag:economics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#economics</a> <a class="tag" href="https://hyunzioh.pages.dev/?query=tag:c-capm" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#c-capm</a> <a class="tag" href="https://hyunzioh.pages.dev/?query=tag:finance" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#finance</a> <br> <a href="https://hyunzioh.pages.dev?query=tag:termstructure" class="tag is-unresolved" target="_self" rel="noopener" data-href="#termstructure">#termstructure</a> <a href="https://hyunzioh.pages.dev?query=tag:economics" class="tag is-unresolved" target="_self" rel="noopener" data-href="#economics">#economics</a> <a href="https://hyunzioh.pages.dev?query=tag:c-capm" class="tag is-unresolved" target="_self" rel="noopener" data-href="#c-capm">#c-capm</a> <a href="https://hyunzioh.pages.dev?query=tag:finance" class="tag is-unresolved" target="_self" rel="noopener" data-href="#finance">#finance</a><br>Oh, Hyunzi. (email: <a data-tooltip-position="top" aria-label="mailto:wisdom302@naver.com" rel="noopener" class="external-link is-unresolved" href="https://hyunzioh.pages.dev/mailto:wisdom302@naver.com" target="_self">wisdom302@naver.com</a>)<br>
Korea University, Graduate School of Economics. <br>Main References<br>Kim, Seung Hyun. (2024). "Asset Pricing and Term Structure Models". WORK IN PROGRESS.
<br>Assume the following notations:<br>: a sequence of payoffs for an asset <br>Here, is understood to be a payoff the investor would receive at time if she were to invest the asset at time and then sell it at . <br>: a price to the asset at time <br>The goal of asset pricing is to find that the price is a function of its payoff , i.e. <br>Definition (rate of return).
The rate of return of the asset at time is defined as where the asset's return at is defined as .
<br>Using <a data-href="#^a14380" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^a14380" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^a14380</a><a data-href="#^a14380" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^a14380" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 1 (rate of return)</a>, we have note that both the denominator () and the numerator () is random variables.<br>the asset is risk-free: the rate of return of the asset is known at time , denoted as .
<br>the asset is risky: the rate of return of the asset () is a random variable. <br>amount of risk of the asset at is represented by .
<br>profitability of the asset at is represented by expected rate of return, .
<br>an asset is said to be high-risk and high-return if and are both large, and it is low-risk and low-return if the both are small. <br>Definition (risk premium).
The risk premium, or expected excess return of an asset at time is defined as which is known at time . The risk premium represents the compensation that an investor receives in exchange for taking on risk. Generally, the riskier the asset, the higher its risk premium.
<br>Assumption (assumptions for CAPM). <br>Many homogeneous investors
<br>Two-period model: the economy lasts for two period of and , where the investors invest at time and receive the payoffs at time .
<br>Many risky assets and one risk-free asset: mean and covariance of the vector of risky asset rate of return are given by where is a positive definite matrix.
<br>Mean-variance utility: given a portfolio with expected rate of return and variance , the representative investor receives utility equal to where denotes the risk-aversion coefficient. <br>Note that a portfolio with weights has <br>rate of return: <br>expected rate of return: <br>risk premium: <br>variance (risk): <br>Thus the problem is to find the vector of weights from the following maximization problem: Then F.O.C. note that is a maximizing solution since the S.O.C. is where the inequality holds as is a positive definite.<br>Thus we have<br>
and the -th row is which represents the risk premium of the -th asset.<br>The optimal portfolio, i.e. the market portfolio has<br>rate of return: <br>expected rate of return: <br>risk premium: <br>risk (variance): From the driven the risk premium of the market portfolio, we have thus the risk-aversion coefficient can be re-written as Furthermore, for any , we have It follows that the risk premium of the -th asset is where and are called beta term and the market price of risk, respectively.
<br>Definition (beta-term).
From the formula of risk premium of asset , the first term is denoted as beta-term which denotes <br>the idiosyncratic part of risk premium of the asset .
<br>the coefficient from the regression of on .
<br>determining how much the factor loads onto the risk premium of asset .
<br>how much represents the market risk(systematic risk).
<br>the of asset added(offset) by the market risk. <br>Definition (market price of risk).
From the formula of risk premium of asset , the second term is denoted as market price of risk which denotes <br>the common part of the risk premium of each asset.
<br>as denotes the risk-aversion coefficient, it means how much the market-risk be represented in the for all . <br>Assumption (assumptions for C-CAPM). <br>Many homogeneous investors.
<br>Multi-period model: the economy starts at times , and is populated by infinitely living investors.
<br>Many risky assets and one risk-free asset: <br>there exists one consumption good, whose price normalized as . <br>there exists risky assets with rate of return at time as and one risk-free asset with rate of return .
<br>at time , the representative investor is assumed to be endowed with unit of asset .
<br>at time , the representative investor holds units of asset .
<br>at time , the price and dividend of asset is denoted as and , respectively. <br>General utility function: the representative investor has instantaneous utility function such that and , implying that the investors are risk-averse and receive positive marginal utilities from consumption. <br>discount factor is given as .
<br>wage income is given as for each period. <br>The investor's problem is<br>The Lagrangian is F.O.C.<br>Thus the Euler equation is which implies that the marginal benefit from the additional consumption at () equals to the discounted expected marginal cost received at .<br>In asset pricing literature, the Euler equation is interpreted as a pricing formula, <br>Definition (stochastic discount factor).
From Euler equation, the price of asset () is given as the expectation of its payoff discounted by the stochastic discount factor (SDF) which denotes <br>the discounted marginal utility of future consumption () relative to the marginal utility of current consumption ().
<br>which gets smaller if the investor expects to consume more tomorrow than today.
<br>which discounts the asset 's payoff much more <br>From the pricing formula, by dividing both sides by , we have Rearranging the equation, we have and the expected rate of return of the risk-free asset is thus Therefore, the risk premium of the asset is where similar to <a data-href="#Capital Asset Pricing Model" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#Capital_Asset_Pricing_Model" class="original-internal-link" target="_self" rel="noopener" style="display: none;">Capital Asset Pricing Model</a><a data-href="#Capital Asset Pricing Model" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#Capital_Asset_Pricing_Model" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Capital Asset Pricing Model</a>, denotes the <a data-href="#^c57404" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^c57404" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^c57404</a><a data-href="#^c57404" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^c57404" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 4 (beta-term)</a> and denotes the <a data-href="#^38ea47" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^38ea47" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^38ea47</a><a data-href="#^38ea47" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^38ea47" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 5 (market price of risk)</a>.<br>Definition (sharp ratio).
The Sharp ratio (SR) of an asset is an indicator of the profitability of an asset relative to its risk. The SR of asset from time to is defined as which is a risk premium (profit) of as asset divided by its risk (volatility, the standard deviation of the rate of return of asset ).<br>
The higher the SR, the grater the the profitability of the asset compared to other assets with the same amount of the risk.
<br>Using the price formula, we can express in term of the correlation between the asset's rate of return and the SDF.<br>
Then, the SR is As by the definition of correlation, we have where the highest and lowest possible SR is the case when the rate of return is perfectly correlated with SDF.<br>Also, note that by the <a data-href="#^fd2d45" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^fd2d45" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^fd2d45</a><a data-href="#^fd2d45" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^fd2d45" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 8 (sharp ratio)</a>, we have which tells us that the expected rate of return of an asset can be understand as the deviation from the risk-free rate of return () by its standard deviation multiplied by the SR. Here, the slope of the line, called as capital allocation line (CAL), is exactly the SR.<br><img alt="Pasted image 20240707231711.png" src="https://hyunzioh.pages.dev/figures/pasted-image-20240707231711.png" style="width: 500px; max-width: 100%;" target="_self"><br>Suppose now that the log of the SDF and the return jointly follow a normal distribution conditional on information up to time :<br>Note that when the random variable follows the normal distribution, the moment generating function is Then the pricing formula is Taking logs on both sides, thus the expected rate of return is <br>Similarly, for the risk-free asset by taking logs on both sides, Therefore, the risk premium of the asset is Note that the additional variance term is denoted as Jensen's Inequality term, and it is often ignored when talking of about the expected excess returns (risk premium).<br>We can also re-define the <a data-href="#^fd2d45" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^fd2d45" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^fd2d45</a><a data-href="#^fd2d45" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^fd2d45" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 8 (sharp ratio)</a> as <br>From <a data-href="#^187b53" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^187b53" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^187b53</a><a data-href="#^187b53" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^187b53" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Assumption 6 (assumptions for C-CAPM)</a>, now assume for the special utility function of CRRA utility: where is the coefficient of relative risk aversion.<br>Then, the <a data-href="#^96b178" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^96b178" class="original-internal-link" target="_self" rel="noopener" style="display: none;">^96b178</a><a data-href="#^96b178" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#^96b178" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Definition 7 (stochastic discount factor)</a> is <br>Defining a consumption growth as , we have where the approximation used the Taylor expansion.<br>Then, the risk premium of asset is and note that often represents the systematic risk presents in the economy, and the beta term (covariance term) shows how much the systematic risk is embedded in the asset itself, and differs from the asset to asset.<br>Under <a data-href="#Case of CRRA Utility" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#Case_of_CRRA_Utility" class="original-internal-link" target="_self" rel="noopener" style="display: none;">Case of CRRA Utility</a><a data-href="#Case of CRRA Utility" href="https://hyunzioh.pages.dev/term-structure/consumption-capm.html#Case_of_CRRA_Utility" class="internal-link mathLink-internal-link" target="_self" rel="noopener">Case of CRRA Utility</a>,]]></description><link>https://hyunzioh.pages.dev/term-structure/consumption-capm.html</link><guid isPermaLink="false">Term Structure/Consumption-CAPM.md</guid><dc:creator><![CDATA[Hyunzi Oh]]></dc:creator><pubDate>Tue, 09 Jul 2024 16:11:23 GMT</pubDate><enclosure url="https://hyunzioh.pages.dev/figures/pasted-image-20240707231711.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://hyunzioh.pages.dev/figures/pasted-image-20240707231711.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Factor Models]]></title><description><![CDATA[<br>Oh, Hyunzi. (email: <a data-tooltip-position="top" aria-label="mailto:wisdom302@naver.com" rel="noopener" class="external-link is-unresolved" href="https://hyunzioh.pages.dev/mailto:wisdom302@naver.com" target="_self">wisdom302@naver.com</a>)<br>
Korea University, Graduate School of Economics.<br>
MoM session: 08, April. 2024.<br><br>Main References<br>
<br>Kim, Seung Hyun. (2024). "Asset Pricing and Term Structure Models".
<br><br><br><br>
<br>Goal: How can we explain the data using components that maximize the signal while minimizing the noise? <br>Signal: unique information stored in each variables, the variance of certain variables.
<br>Noise: redundant (overlapping) information in a certain variables, the covariance of the variables with others. <br>Usage: To extract the co-factors that can explain the group of variables. <br>(ex; Output) multiple indices, but shared characteristics?
<br>(ex; Price) hundreds of items, but driven by a small number of factors? <br>Limitation: Can distort the original information. <br>How should we interpret the extracted components?
<br>Might not provide a stronger explanatory power than just a weighted sum or arbitrarily selected variables. <br><br>Let the population data is given as then the population covariance matrix is which positive definite.<br>Our goal is to find that are the linear combinations of variables, such that<br>
<br>maximizes , where <br>each () are uncorrelated, i.e. <br>coefficients are normalized to solve the problem, i.e. <br><br>We need to solve First, we calculate : then the Lagrangian function is F.O.C. thus is a unit eigenvector of that corresponds to the largest eigenvalue .<br>Next, we calculate : then the Lagrangian function is F.O.C. note that since , we have , as we are assuming that . therefore we have thus we have , i.e. is a unit eigenvector of that corresponds to that is orthogonal to .<br>Similarly, we can derive PCs, such that corresponds to .<br><br>
<br>Number of PCs is : where are the orthonormal eigenbasis.
<br>Let , then we have <br>Strength of -th PC: .
<br><br>Let be the given sample data: i.e. for each variable , we have samples.<br>
<br>Calculate the sample covariance, which will converge in probability to by Law of Large Numbers.
<br>Choose the orthogonal eigenbasis of with ordered eigenvalues , so that <br>Since are sample variances of each sample PC, we usually normalize by <br>Finally, as the orthogonal eigenvectors of the positive definite matrix are not unique, apply additional restrictions to identify PCs.
<br><br><br><br>
<br>Goal: find the small number of factors that can explain the co-movement of the economic variables.
<br>Static Factor Model: dynamics of the factors ignored.
<br>exact factor model: completely explains cross-sectional and temporal co-movement, while is purely idiosyncratic.
<br>approximate factor model: limited correlation among is allowed.
<br><br>Let the data be matrix: or equivalently, note that .<br>LSAFM follows in matrix form: or equivalently, <br>In vector form: or equivalently, <br>Then PC estimator of minimizes the objective function : <br><br>The PC estimator can be driven in either two approaches:<br>
<br>First concentrate out the factor loadings, and then obtain the factors.
<br>First concentrate out the factors, and then obtain the loadings.
<br>For the convenience of the matrix computations, the first approach is preferred when , and the later one is often used when . We only look for the second method, but the first approach follows the similar proceedings to the second one. Furthermore, the PCs obtained from either approach are closely related, which are shown in Bai and Ng (2002).<br><br>From the objective function , we obtain Thus the factor estimates are The concentrated objective function is <br><br>We impose a restriction to identify the factor loadings: .<br>
Given , the minimization problem is which is same as solving since is positive semidefinite and , we have where is eivenvalues of .<br>Thus the minimized objective function is <br><br>
<br>Factor loadings: = orthonormal eigenvectors corresponding to eigenvalues of .
<br>Factors: .
<br>.
<br><br>
<br>(MSE) find such that minimize <br>(ER) find that maximize .
<br>(PIC) find that minimize , where commonly we use .
<br>]]></description><link>https://hyunzioh.pages.dev/econometrics/factor-models.html</link><guid isPermaLink="false">Econometrics/Factor Models.md</guid><dc:creator><![CDATA[Hyunzi Oh]]></dc:creator><pubDate>Tue, 21 May 2024 11:42:05 GMT</pubDate></item></channel></rss>